{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMitWfoTW8nqA2pw1xQXaPt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
        },
        "8619e18844f840f7b80442b01a7f9d5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b759998cad044b32b9c0dd2d243a4ae8",
            "placeholder": "​",
            "style": "IPY_MODEL_06587973cbff48aab57306178a746612",
            "value": "model.safetensors: 100%"
          }
        },
        "f426a6e3ea1d4a8dab3f63e6d8f7969d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01801771bfd048f9af1d83568bb0841c",
            "max": 331055963,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9b5612c91c79476d951ba27593ea87e7",
            "value": 331055963
          }
        },
        "aa55b7c629a947c191e4f3058be77fdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7907846425f4d6aa173a65c76805234",
            "placeholder": "​",
            "style": "IPY_MODEL_21f42ec29c544abf8dcc1cd7ed1ccfd4",
            "value": " 331M/331M [00:04&lt;00:00, 120MB/s]"
          }
        },
        "f8ca8c334fba4145ad95c06d1b832542": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b759998cad044b32b9c0dd2d243a4ae8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06587973cbff48aab57306178a746612": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "01801771bfd048f9af1d83568bb0841c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b5612c91c79476d951ba27593ea87e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c7907846425f4d6aa173a65c76805234": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21f42ec29c544abf8dcc1cd7ed1ccfd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/usp787/DS_5110_Final_Project_LoRA/blob/Code/LoRA_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "iRw49Jo0RQ4j"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_goemotions_data():\n",
        "    \"\"\"Load GoEmotions dataset with pre-split train/val/test\"\"\"\n",
        "    print(\"\\nLoading GoEmotions dataset...\")\n",
        "    dataset = load_dataset('google-research-datasets/go_emotions', 'simplified')\n",
        "\n",
        "    print(f\"Train samples: {len(dataset['train']):,}\")\n",
        "    print(f\"Validation samples: {len(dataset['validation']):,}\")\n",
        "    print(f\"Test samples: {len(dataset['test']):,}\")\n",
        "\n",
        "    return dataset['train'], dataset['validation'], dataset['test']"
      ],
      "metadata": {
        "id": "SjyMOmaCSw3X"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_batch(batch_data, tokenizer, max_length=128):\n",
        "    \"\"\"\n",
        "    Convert raw text batch to model inputs\n",
        "    More flexible than Dataset class - easy to modify tokenization\n",
        "    \"\"\"\n",
        "    texts = [item['text'] for item in batch_data]\n",
        "\n",
        "    # Tokenize\n",
        "    encoding = tokenizer(\n",
        "        texts,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    # Multi-label: Convert to binary vectors\n",
        "    labels = torch.zeros(len(batch_data), 28)\n",
        "    for i, item in enumerate(batch_data):\n",
        "        for label_id in item['labels']:\n",
        "            labels[i, label_id] = 1\n",
        "\n",
        "    return encoding['input_ids'], encoding['attention_mask'], labels"
      ],
      "metadata": {
        "id": "uVmoVjHgS5Gm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloaders(train_data, val_data, test_data, tokenizer, batch_size=32, max_length=128):\n",
        "    \"\"\"\n",
        "    Create dataloaders from raw data\n",
        "    Using simple Dataset wrapper for DataLoader compatibility\n",
        "    \"\"\"\n",
        "    class SimpleDataset(Dataset):\n",
        "        def __init__(self, data):\n",
        "            self.data = data\n",
        "        def __len__(self):\n",
        "            return len(self.data)\n",
        "        def __getitem__(self, idx):\n",
        "            return self.data[idx]\n",
        "\n",
        "    # Wrap in Dataset for DataLoader\n",
        "    train_dataset = SimpleDataset(train_data)\n",
        "    val_dataset = SimpleDataset(val_data)\n",
        "    test_dataset = SimpleDataset(test_data)\n",
        "\n",
        "    # Custom collate function\n",
        "    def collate_fn(batch):\n",
        "        input_ids, attention_mask, labels = prepare_batch(batch, tokenizer, max_length)\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': labels\n",
        "        }\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "CYeK8GEcTAqx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import peft librareis(Hugging Face) for LoRA\n",
        "from peft import LoraConfig, get_peft_model, TaskType"
      ],
      "metadata": {
        "id": "NRtNNrcEcnnE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "    #set rank=16\n",
        "def build_model(num_labels=28, dropout=0.1, rank=16):\n",
        "    \"\"\"\n",
        "    Build DistilRoBERTa with LoRA adapters.\n",
        "    \"\"\"\n",
        "    # 1. Load the pre-trained backbone (Standard AutoModel)\n",
        "    # This is currently FROZEN (all weights)\n",
        "    backbone = AutoModel.from_pretrained('distilroberta-base')\n",
        "\n",
        "    # 2. Configure LoRA\n",
        "    # We target the attention mechanism linear layers.\n",
        "    # DistilRoBERTa uses 'key', 'query', 'value' in its attention modules.\n",
        "    peft_config = LoraConfig(\n",
        "        task_type=TaskType.FEATURE_EXTRACTION, # We are using it as a feature extractor for our head\n",
        "        r=rank,                  # The dimension of the low-rank matrices (8, 16, 64)\n",
        "        lora_alpha=32,           # Scaling factor (usually 2x rank)\n",
        "        lora_dropout=0.1,        # Regularization\n",
        "        target_modules=['key', 'query', 'value'] # Specific to RoBERTa-style models\n",
        "    )\n",
        "\n",
        "    # 3. Inject Adapters (The Magic Step)\n",
        "    # This makes the backbone's adapters TRAINABLE, while keeping the rest frozen.\n",
        "    backbone = get_peft_model(backbone, peft_config)\n",
        "    print(\"\\n✓ LoRA Adapters injected into Backbone\")\n",
        "    backbone.print_trainable_parameters() # Helpful built-in print function\n",
        "\n",
        "    # 4. Define the Classifier Head (Same as before)\n",
        "    # The backbone now returns adapted embeddings!\n",
        "    class EmotionClassifier(nn.Module):\n",
        "        def __init__(self, backbone, classifier):\n",
        "            super().__init__()\n",
        "            self.backbone = backbone\n",
        "            self.classifier = classifier\n",
        "\n",
        "        def forward(self, input_ids, attention_mask):\n",
        "            # The backbone handles the LoRA logic internally\n",
        "            outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            # Use [CLS] token representation\n",
        "            pooled = outputs.last_hidden_state[:, 0, :]\n",
        "            logits = self.classifier(pooled)\n",
        "            return logits\n",
        "\n",
        "    # Re-create your classifier head\n",
        "    hidden_size = backbone.config.hidden_size\n",
        "    classifier_head = nn.Sequential(\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(hidden_size, hidden_size // 2),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(hidden_size // 2, num_labels)\n",
        "    )\n",
        "\n",
        "    # Combine them\n",
        "    model = EmotionClassifier(backbone, classifier_head)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "1p5vAf_CTB6u"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(predictions, labels, threshold=0.5):\n",
        "\n",
        "    # Binarize predictions\n",
        "    pred_binary = (predictions >= threshold).astype(int)\n",
        "    labels = labels.astype(int)\n",
        "\n",
        "    # MICRO metrics: Global aggregation\n",
        "    micro_f1 = f1_score(labels, pred_binary, average='micro', zero_division=0)\n",
        "    micro_precision = precision_score(labels, pred_binary, average='micro', zero_division=0)\n",
        "    micro_recall = recall_score(labels, pred_binary, average='micro', zero_division=0)\n",
        "\n",
        "    # MACRO metrics: Per-class average\n",
        "    macro_f1 = f1_score(labels, pred_binary, average='macro', zero_division=0)\n",
        "    macro_precision = precision_score(labels, pred_binary, average='macro', zero_division=0)\n",
        "    macro_recall = recall_score(labels, pred_binary, average='macro', zero_division=0)\n",
        "\n",
        "    # Additional metrics\n",
        "    subset_acc = accuracy_score(labels, pred_binary)  # Exact match\n",
        "    hamming = np.mean(labels != pred_binary)  # Label-wise error\n",
        "\n",
        "    return {\n",
        "        'micro_f1': micro_f1,\n",
        "        'micro_precision': micro_precision,\n",
        "        'micro_recall': micro_recall,\n",
        "        'macro_f1': macro_f1,\n",
        "        'macro_precision': macro_precision,\n",
        "        'macro_recall': macro_recall,\n",
        "        'subset_accuracy': subset_acc,\n",
        "        'hamming_loss': hamming\n",
        "    }\n",
        "\n",
        "\n",
        "def print_metrics(metrics, prefix=\"\"):\n",
        "    \"\"\"Pretty print metrics\"\"\"\n",
        "    print(f\"\\n{prefix}Metrics:\")\n",
        "    print(f\"  Micro-F1: {metrics['micro_f1']:.4f} (weighted by frequency)\")\n",
        "    print(f\"  Macro-F1: {metrics['macro_f1']:.4f} (equal weight per class)\")\n",
        "    print(f\"  Micro-Precision: {metrics['micro_precision']:.4f}\")\n",
        "    print(f\"  Micro-Recall: {metrics['micro_recall']:.4f}\")\n",
        "    print(f\"  Macro-Precision: {metrics['macro_precision']:.4f}\")\n",
        "    print(f\"  Macro-Recall: {metrics['macro_recall']:.4f}\")\n",
        "    print(f\"  Subset Accuracy: {metrics['subset_accuracy']:.4f}\")\n",
        "    print(f\"  Hamming Loss: {metrics['hamming_loss']:.4f}\")"
      ],
      "metadata": {
        "id": "lDSwoA0MTfc0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    \"\"\"\n",
        "    Train for one epoch\n",
        "    Separate function - easy to modify training logic\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
        "    for batch in progress_bar:\n",
        "        # Move to device\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        # Forward\n",
        "        logits = model(input_ids, attention_mask)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track loss\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "flP9vesWTmXj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataloader, criterion, device):\n",
        "    \"\"\"\n",
        "    Evaluate model on a dataset\n",
        "    Returns loss + all metrics\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            # Move to device\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Forward\n",
        "            logits = model(input_ids, attention_mask)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            # Get predictions (sigmoid for multi-label)\n",
        "            probs = torch.sigmoid(logits)\n",
        "\n",
        "            # Collect\n",
        "            all_predictions.append(probs.cpu().numpy())\n",
        "            all_labels.append(labels.cpu().numpy())\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    # Aggregate\n",
        "    predictions = np.vstack(all_predictions)\n",
        "    labels = np.vstack(all_labels)\n",
        "\n",
        "    # Compute metrics\n",
        "    metrics = compute_metrics(predictions, labels)\n",
        "    metrics['loss'] = total_loss / len(dataloader)\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "s20V8oUaTpeP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, device, epochs=5, lr=1e-3):\n",
        "    \"\"\"\n",
        "    Main training loop\n",
        "    Easy to modify hyperparameters and logic\n",
        "    \"\"\"\n",
        "    # Setup\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(\n",
        "        filter(lambda p: p.requires_grad, model.parameters()),\n",
        "        lr=lr\n",
        "    )\n",
        "\n",
        "    best_val_macro_f1 = 0\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Training Loop\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Train\n",
        "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        print(f\"Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "        # Validate\n",
        "        val_metrics = evaluate_model(model, val_loader, criterion, device)\n",
        "        print(f\"Val Loss: {val_metrics['loss']:.4f}\")\n",
        "        print_metrics(val_metrics, prefix=\"Validation \")\n",
        "\n",
        "        # Save best model\n",
        "        if val_metrics['macro_f1'] > best_val_macro_f1:\n",
        "            best_val_macro_f1 = val_metrics['macro_f1']\n",
        "            torch.save(model.state_dict(), 'best_LoRA_model.pt')\n",
        "            print(f\"  ✓ New best model saved! (Macro-F1: {best_val_macro_f1:.4f})\")\n",
        "\n",
        "    return best_val_macro_f1"
      ],
      "metadata": {
        "id": "tw1e71G6TtW6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Main execution function\n",
        "    Each step is separate - easy to run/modify individually\n",
        "    \"\"\"\n",
        "    # Config\n",
        "    BATCH_SIZE = 32\n",
        "    MAX_LENGTH = 128\n",
        "    EPOCHS = 5\n",
        "    LR = 1e-3\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"DistilRoBERTa LoRA\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Device: {DEVICE}\\n\")\n",
        "\n",
        "    # Step 1: Load data\n",
        "    train_data, val_data, test_data = load_goemotions_data()\n",
        "\n",
        "    # Step 2: Load tokenizer\n",
        "    print(\"\\nLoading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained('distilroberta-base')\n",
        "\n",
        "    # Step 3: Create dataloaders\n",
        "    print(\"\\nCreating dataloaders...\")\n",
        "    train_loader, val_loader, test_loader = create_dataloaders(\n",
        "        train_data, val_data, test_data, tokenizer,\n",
        "        batch_size=BATCH_SIZE, max_length=MAX_LENGTH\n",
        "    )\n",
        "\n",
        "    # Step 4: Build model\n",
        "    print(\"\\nBuilding model...\")\n",
        "    model = build_model(num_labels=28, dropout=0.1)\n",
        "    model = model.to(DEVICE)\n",
        "\n",
        "    # Step 5: Train\n",
        "    best_macro_f1 = train_model(\n",
        "        model, train_loader, val_loader, DEVICE,\n",
        "        epochs=EPOCHS, lr=LR\n",
        "    )\n",
        "\n",
        "    # Step 6: Final test evaluation\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Final Test Evaluation (Unseen Data)\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    model.load_state_dict(torch.load('best_LoRA_model.pt'))\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    test_metrics = evaluate_model(model, test_loader, criterion, DEVICE)\n",
        "\n",
        "    print_metrics(test_metrics, prefix=\"Test \")\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"LoRA Complete!\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"\\nKey Results:\")\n",
        "    print(f\"  Best Val Macro-F1: {best_macro_f1:.4f}\")\n",
        "    print(f\"  Test Macro-F1: {test_metrics['macro_f1']:.4f}\")\n",
        "    print(f\"  Test Micro-F1: {test_metrics['micro_f1']:.4f}\")\n",
        "    #print(f\"\\nReady for LoRA comparison!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3ac07072fcb74032bcdf85163ad0ee6e",
            "8619e18844f840f7b80442b01a7f9d5d",
            "f426a6e3ea1d4a8dab3f63e6d8f7969d",
            "aa55b7c629a947c191e4f3058be77fdd",
            "f8ca8c334fba4145ad95c06d1b832542",
            "b759998cad044b32b9c0dd2d243a4ae8",
            "06587973cbff48aab57306178a746612",
            "01801771bfd048f9af1d83568bb0841c",
            "9b5612c91c79476d951ba27593ea87e7",
            "c7907846425f4d6aa173a65c76805234",
            "21f42ec29c544abf8dcc1cd7ed1ccfd4"
          ]
        },
        "id": "Ls-6pgHNU8ks",
        "outputId": "50832dfe-8c77-4e8a-af72-351c4a52a118"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "DistilRoBERTa LoRA\n",
            "============================================================\n",
            "Device: cuda\n",
            "\n",
            "\n",
            "Loading GoEmotions dataset...\n",
            "Train samples: 43,410\n",
            "Validation samples: 5,426\n",
            "Test samples: 5,427\n",
            "\n",
            "Loading tokenizer...\n",
            "\n",
            "Creating dataloaders...\n",
            "\n",
            "Building model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ac07072fcb74032bcdf85163ad0ee6e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ LoRA Adapters injected into Backbone\n",
            "trainable params: 442,368 || all params: 82,560,768 || trainable%: 0.5358\n",
            "\n",
            "============================================================\n",
            "Training Loop\n",
            "============================================================\n",
            "\n",
            "Epoch 1/5\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1357/1357 [05:07<00:00,  4.41it/s, loss=0.0611]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 170/170 [00:17<00:00,  9.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.0930\n",
            "\n",
            "Validation Metrics:\n",
            "  Micro-F1: 0.4821 (weighted by frequency)\n",
            "  Macro-F1: 0.3918 (equal weight per class)\n",
            "  Micro-Precision: 0.6952\n",
            "  Micro-Recall: 0.3690\n",
            "  Macro-Precision: 0.6107\n",
            "  Macro-Recall: 0.3307\n",
            "  Subset Accuracy: 0.3421\n",
            "  Hamming Loss: 0.0333\n",
            "  ✓ New best model saved! (Macro-F1: 0.3918)\n",
            "\n",
            "Epoch 2/5\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1357/1357 [05:08<00:00,  4.41it/s, loss=0.0651]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0920\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 170/170 [00:17<00:00,  9.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.0877\n",
            "\n",
            "Validation Metrics:\n",
            "  Micro-F1: 0.5277 (weighted by frequency)\n",
            "  Macro-F1: 0.3874 (equal weight per class)\n",
            "  Micro-Precision: 0.7454\n",
            "  Micro-Recall: 0.4085\n",
            "  Macro-Precision: 0.6961\n",
            "  Macro-Recall: 0.3174\n",
            "  Subset Accuracy: 0.3878\n",
            "  Hamming Loss: 0.0307\n",
            "\n",
            "Epoch 3/5\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1357/1357 [05:08<00:00,  4.40it/s, loss=0.0850]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0896\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 170/170 [00:17<00:00,  9.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.0873\n",
            "\n",
            "Validation Metrics:\n",
            "  Micro-F1: 0.5675 (weighted by frequency)\n",
            "  Macro-F1: 0.4374 (equal weight per class)\n",
            "  Micro-Precision: 0.6940\n",
            "  Micro-Recall: 0.4799\n",
            "  Macro-Precision: 0.6677\n",
            "  Macro-Recall: 0.3797\n",
            "  Subset Accuracy: 0.4445\n",
            "  Hamming Loss: 0.0307\n",
            "  ✓ New best model saved! (Macro-F1: 0.4374)\n",
            "\n",
            "Epoch 4/5\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1357/1357 [05:07<00:00,  4.41it/s, loss=0.0755]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0883\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 170/170 [00:17<00:00,  9.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.0862\n",
            "\n",
            "Validation Metrics:\n",
            "  Micro-F1: 0.5527 (weighted by frequency)\n",
            "  Macro-F1: 0.4228 (equal weight per class)\n",
            "  Micro-Precision: 0.7054\n",
            "  Micro-Recall: 0.4544\n",
            "  Macro-Precision: 0.6622\n",
            "  Macro-Recall: 0.3538\n",
            "  Subset Accuracy: 0.4300\n",
            "  Hamming Loss: 0.0309\n",
            "\n",
            "Epoch 5/5\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1357/1357 [05:07<00:00,  4.41it/s, loss=0.1046]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0880\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 170/170 [00:17<00:00,  9.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.0896\n",
            "\n",
            "Validation Metrics:\n",
            "  Micro-F1: 0.5256 (weighted by frequency)\n",
            "  Macro-F1: 0.4169 (equal weight per class)\n",
            "  Micro-Precision: 0.7078\n",
            "  Micro-Recall: 0.4180\n",
            "  Macro-Precision: 0.6669\n",
            "  Macro-Recall: 0.3579\n",
            "  Subset Accuracy: 0.3848\n",
            "  Hamming Loss: 0.0317\n",
            "\n",
            "============================================================\n",
            "Final Test Evaluation (Unseen Data)\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 170/170 [00:17<00:00,  9.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Metrics:\n",
            "  Micro-F1: 0.5618 (weighted by frequency)\n",
            "  Macro-F1: 0.4162 (equal weight per class)\n",
            "  Micro-Precision: 0.6996\n",
            "  Micro-Recall: 0.4694\n",
            "  Macro-Precision: 0.6295\n",
            "  Macro-Recall: 0.3595\n",
            "  Subset Accuracy: 0.4391\n",
            "  Hamming Loss: 0.0305\n",
            "\n",
            "============================================================\n",
            "LoRA Complete!\n",
            "============================================================\n",
            "\n",
            "Key Results:\n",
            "  Best Val Macro-F1: 0.4374\n",
            "  Test Macro-F1: 0.4162\n",
            "  Test Micro-F1: 0.5618\n"
          ]
        }
      ]
    }
  ]
}
